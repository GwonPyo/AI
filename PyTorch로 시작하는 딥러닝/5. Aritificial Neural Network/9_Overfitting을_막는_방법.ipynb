{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9. Overfitting을 막는 방법.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 1. 데이터의 양 늘리기 </strong>"
      ],
      "metadata": {
        "id": "KZ1_hm2ACVxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터가 적으면 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 학습하게 되어 과적합 현상이 발생할 확률이 높아진다. <br>\n",
        "만약 데이터 양이 적다면 기존 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘릴 수 있다. <br>\n",
        "이러한 방식을 **Data Augmentation**이라고 부른다."
      ],
      "metadata": {
        "id": "X_4X2g2YCktz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 2. 모델의 복잡도 줄이기 </strong>"
      ],
      "metadata": {
        "id": "FlUKL6SODCGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "인공 신경망의 복잡도는 은닉층의 수나 매개변수의 수 등으로 결정된다. <br>\n",
        "모델의 복잡도를 줄이면 과적합 현상을 어느 정도 케어할 수 있다.\n",
        "\n",
        "* 모델에 있는 매개변수들의 수를 모델의 **수용력(capacity)**라고 부른다."
      ],
      "metadata": {
        "id": "KtcRDKNoDGwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 3. 가중치 규제 적용 </strong>"
      ],
      "metadata": {
        "id": "lQAxaa5JDTH4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "복잡한 모델은 과적합될 가능성이 높다. <br>\n",
        "복잡한 모델을 조금 더 간단하게 하는 방법으로 **가중치 규제(Regularization)**이 있다.\n",
        "\n",
        "* L1 규제: 가중치 w들의 절대값 합계를 비용 함수에 추가한다.\n",
        "* L2 규제: 가중치 w들의 제곱합을 비용 함수에 추가한다.\n",
        "\n",
        "L1 규제는 기존 비용 함수의 모든 가중치에 대해 $λ|w|$를 더한 값을 비용 함수로 한다. <br>\n",
        "반면 L2 규제는$\\frac{1}{2}λw^2$를 더한 값을 비용 함수로 사용한다. <br>\n",
        "이때 $λ$는 규제의 강도를 정하는 하이퍼 파라미터이다.\n",
        "\n",
        "두 식 모두 비용 함수를 최소화하기 위해 가중치 w들의 값이 작아져야 한다. <br>\n",
        "이때 L1 규제는 w의 값들이 0 또는 0에 가까워진다. <br> 즉, 값 자체가 0이 될 수 있어 해당 가중치에 곱해지는 특성은 모델의 결과에 별 영향을 주지 못한다. <br>\n",
        "따라서 **L1 규제는 어떤 특성들이 모델에 영향을 주는지 정확히 판단할 때 유용하다.**\n",
        "\n",
        "하지만 L2 규제는 가중치의 제곱을 최소화하여 w의 값이 완전히 0이 되기보다는 0에 가까워지는 경향을 보인다. <br>\n",
        "일반적으로 **L1 규제보다 L2 규제가 더 잘 작동**하며 L2 규제를 **가중치 감쇠(weight decay)**라고도 부른다.\n",
        "\n",
        "파이토치에서는 옵티마이저의 **weight_decay 매개변수**를 설정해 L2 규제를 적용한다.\n",
        "\n",
        "```\n",
        "model = Architecture1(10, 20, 2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "```\n"
      ],
      "metadata": {
        "id": "Jb3gmKtnDgOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 4. 드롭아웃 </strong>"
      ],
      "metadata": {
        "id": "lfzwtbRFHKDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다. <br>\n",
        "예를 들어, 드롭아웃의 비율을 0.5로 지정하며 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않는다. <br>\n",
        "**단, 학습시에만 사용하고, 예측 시에는 사용하지 않는다.**\n",
        "\n",
        "드롭아웃은 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해준다. <br>\n",
        "또한 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지한다."
      ],
      "metadata": {
        "id": "RUFmvYY0HZoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출처: https://wikidocs.net/60751"
      ],
      "metadata": {
        "id": "oxg7icTeH53I"
      }
    }
  ]
}