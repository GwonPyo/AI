{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Softmax Regression Principle.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 1. 다중 클래스 분류 </strong>\n",
        "\n"
      ],
      "metadata": {
        "id": "56RGCcda5gKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "세 개 이상의 답 중 하나를 고르는 문제를 **다중 클래스 분류(Multi-class Classification)**이라고 한다."
      ],
      "metadata": {
        "id": "-ZNu4v_c6D7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> <strong> 1.1 로지스틱 회귀 </strong> </h3>\n",
        "\n",
        "로지스틱 회귀에서 시그모이드 함수는 예측값을 0과 1사이의 값으로 만든다. <br>\n",
        "예를 들어 스펨 메일 분류기 모델에서 출력이 0.75이라면 스펨일 확률이 75%라는 의미이다.\n",
        "\n",
        "$H(x) = sigmoid(Wx+B)$"
      ],
      "metadata": {
        "id": "4fy7haNt7Tga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> <strong> 1.2 소프트맥스 회귀 </strong> </h3>\n",
        "\n",
        "**소프트맥스 회귀(Softmax Regression)**는 확률의 총합이 1이 되는 아이디어를 다중 클래스 분류 문제에 적용한다. <br>\n",
        "즉, 소프트맥스 회귀는 선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 해당 벡터의 모든 원소의 합이 1이 되도록 값을 변환시키는 함수를 지나게 만들어야 한다. <br>\n",
        "이때 사용할 함수가 바로 **소프트맥스(softmax)** 함수이다.\n",
        "\n",
        "$H(x)=softmax(WX+B)$"
      ],
      "metadata": {
        "id": "b76A8b2h74sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 2. 소프트맥스 함수 </strong>"
      ],
      "metadata": {
        "id": "2CQG2NR2ci4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> <strong> 2.1 소프트맥스 함수의 이해 </strong> </h3>\n",
        "\n",
        "k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$라고 할 때 소프트맥스 함수는 $p_i$를 다음과 같이 정의한다.\n",
        "\n",
        "$p_i=\\frac{e^{z_i}}{\\sum_{j=1}^{k}e^{z_j}} \\, (for i=1, 2, \\dots, k)$\n",
        "\n",
        "4개의 feature를 가지고 3개의 클래스로 분류하는 문제를 생각해보자. <br> \n",
        "총 클래스는 3개이므로 소프트맥스 함수는 3차원 벡터 $[z_1, z_2, z_3]$를 입력받고 아래와 같은 출력을 한다.\n",
        "\n",
        "$softmax(z)=[\\frac{e^{z_1}}{\\sum_{j=1}^{3}e^{z_j}}, \\frac{e^{z_2}}{\\sum_{j=1}^{3}e^{z_j}}, \\frac{e^{z_3}}{\\sum_{j=1}^{3}e^{z_j}}] = [p_1, p_2, p_3]$\n"
      ],
      "metadata": {
        "id": "PjsSd1KTclDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> <strong> 2.2 행렬 연산 </strong> </h3>\n",
        "\n",
        "4개의 feature를 가지고 3개의 클래스를 분류할 때 행렬 연산을 생각해보자. <br>\n",
        "셈플의 개수가 5개인 경우 입력은 $ 5\\times 4$가 된다. <br>\n",
        "이때 우리가 원하는 출력 벡터의 크기는 $5 \\times 3$이다. <br>\n",
        "이때 가중치 W의 크기는 $4 \\times 3$, 편향 B의 크기는 출력의 크기와 동일한 $5 \\times 3$가 된다.\n",
        "\n",
        "$\n",
        "\\begin{pmatrix}\n",
        "y_{11} \\, y_{12} \\, y_{13} \\\\\n",
        "y_{21} \\, y_{22} \\, y_{23} \\\\\n",
        "y_{31} \\, y_{32} \\, y_{33} \\\\\n",
        "y_{41} \\, y_{42} \\, y_{43} \\\\\n",
        "y_{51} \\, y_{52} \\, y_{53} \\\\\n",
        "\\end{pmatrix}=\n",
        "softmax \n",
        "\\begin{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "x_{11} \\, x_{12} \\, x_{13}, x_{14} \\\\\n",
        "x_{21} \\, x_{22} \\, x_{23}, x_{24} \\\\\n",
        "x_{31} \\, x_{32} \\, x_{33}, x_{34} \\\\\n",
        "x_{41} \\, x_{42} \\, x_{43}, x_{44} \\\\\n",
        "x_{51} \\, x_{52} \\, x_{53}, x_{54} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_{11} \\, w_{12} \\, w_{13} \\\\\n",
        "w_{21} \\, w_{22} \\, w_{23} \\\\\n",
        "w_{31} \\, w_{32} \\, w_{33} \\\\\n",
        "w_{41} \\, w_{42} \\, w_{43} \\\\\n",
        "\\end{pmatrix}\n",
        "+\n",
        "\\begin{pmatrix}\n",
        "b_{11} \\, b_{12} \\, b_{13} \\\\\n",
        "b_{21} \\, b_{22} \\, b_{23} \\\\\n",
        "b_{31} \\, b_{32} \\, b_{33} \\\\\n",
        "b_{41} \\, b_{42} \\, b_{43} \\\\\n",
        "b_{51} \\, b_{52} \\, b_{53} \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{pmatrix}\n",
        "$"
      ],
      "metadata": {
        "id": "_G2sh5N1hMQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 3. 비용함수 </strong>"
      ],
      "metadata": {
        "id": "luwvM8HVktEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> <strong> 3.1 크로스 엔트로피 함수 </strong> </h3>\n",
        "\n",
        "아래에서 $y$는 실제값을 의미하며, k는 클래스의 개수이다. <br>\n",
        "$y_j$는 실제값 원-핫 벡터의 j번째 인덱스를 의미하고, $p_j$는 샘플 데이터가 j번째 클래스일 확률을 나타낸다. <br>\n",
        "$p_j$는 $\\hat{y_j}$로도 표현할 수 있다.\n",
        "\n",
        "$cost(W) = -\\sum_{j=1}^{k}{y_jlog(p_j)}$\n",
        "\n",
        "c가 정답일 때 원-핫 벡터는 c에 해당하는 인덱스의 원소만 1을 가지고 나머지 원소는 모두 0이다. <br>\n",
        "**즉, 위 함수는 정답의 예측값만을 이용해 오차를 계산한다.** <br>\n",
        "$log$함수는 0에 가까울 수록 음의 무한대에 가깝고 1에 가까울 수록 0에 가까워지므로 손실 함수에 적합하다. <br>\n",
        "\n",
        "위 함수를 n개의 전체 데이터에 평균을 구하면 최종 비용 함수는 다음과 같다.\n",
        "\n",
        "$cost(W) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{k}{y_j^{(i)}log(p_j^{(i)})}$"
      ],
      "metadata": {
        "id": "NO_nbTXZkwfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3> <strong> 3.2 이진 분류에서의 크로스 엔트로피 함수 </strong> </h3>\n",
        "\n",
        "로지스틱 회귀에서 배운 크로스 엔트로피 함수식과 위 식은 본질적으로 동일하다.\n",
        "\n",
        "$cost(W) = -[ylog(H(x))+(1-y)log(1-H(x))]$\n",
        "\n",
        "위 식에서 $H(x)$는 $p_1$, $1-H(x)$는 $p_2$로 치환해보자. <br>\n",
        "그래고 $y$는 $y_1$, $1-y$는 $y_2$로 치환하면 아래 식을 얻을 수 있다.\n",
        "\n",
        "$-(y_1log(p_1)+y_2log(p_2))=-(\\sum_{i=1}^{2}log(p_i))$\n",
        "\n",
        "위 식을 일반화하면 아래와 같다.\n",
        "\n",
        "$-\\sum_{i=1}^{k}{y_ilog(p_i)}$\n",
        "\n",
        "즉, **바이너리 크로스 엔트로피 함수와 크로스 엔트로피 함수의 본질적 의미는 동일하다.**"
      ],
      "metadata": {
        "id": "3h_meOhtnor_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출처: https://wikidocs.net/59427"
      ],
      "metadata": {
        "id": "1zazK-qQpjXu"
      }
    }
  ]
}