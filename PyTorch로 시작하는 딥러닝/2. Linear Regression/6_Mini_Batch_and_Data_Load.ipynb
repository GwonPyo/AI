{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Mini Batch and Data Load.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 1. 미니 배치와 배치 크기 </strong>"
      ],
      "metadata": {
        "id": "7KzHNDVyfdOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다중 선형 회귀에서 사용한 데이터를 보자."
      ],
      "metadata": {
        "id": "swCuao2CfqwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "metadata": {
        "id": "4c3zuWJBfv8C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전체 5개의 데이터를 하나의 행렬로 선언하여 전체 데이터에 대해 경사 하강법을 수행하여 학습했다. <br>\n",
        "하지만 데이터가 커졌을 때 위와 같이 전체 데이터에 대해 경사 하강법을 수행하면 너무 많은 계산량이 필요하다. <br>\n",
        "\n",
        "따라서 전체 데이터를 작은 단위로 나눠서 학습해야 하는데 이 단위를 **미니 배치(Mini Batch)**라고 한다. <br>\n",
        "\n",
        "미니 배치 학습에서는 미니 배치의 개수만큼 경사 하강법을 수행해야 전체 데이터가 전부 사용되어 1 Epoch가 된다. <br>\n",
        "미니 배치의 개수는 미니 배치의 크기에 따라 달라지는데 이때 미니 배치의 크기를 **batch size**라고 부른다.\n",
        "\n",
        "* 배치 크기는 **보통 2의 제곱수를 사용한다.** <br> \n",
        "cpu와 gpu의 메모리가 2의 배수이므로 배치 크기가 2의 제곱수일 경우 데이터 송수신의 효율을 높일 수 있기 때문이다.\n",
        "\n",
        "* 미니 배치 경사 하강법은 전체 데이터의 일부를 보고 수행하므로 **최적값 수렴 과정에서 조금 헤매기도 하지만 훈련 속도가 빠르다.**"
      ],
      "metadata": {
        "id": "JdFJZcSKfx98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 2. 이터레이션 </strong>"
      ],
      "metadata": {
        "id": "PhhY-0NBjAht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**이터레이션(Iteration)**이란 한 번의 에포크 내에서 이루어지는 매개변수 가중치 W와 b의 업데이터 횟수이다. <br>\n",
        "전체 데이터가 2,000일 때 batch size가 200이라면 총 이터레이션은 10개이다."
      ],
      "metadata": {
        "id": "dXQ0m7mHjH9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 3. 데이터 로드하기 </strong>"
      ],
      "metadata": {
        "id": "27WAPR0-jXB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "파이토치에서는 데이터를 좀 더 쉽게 다룰 수 있도록 **데이터셋(Dataset)**과 **데이터로더(DataLoader)**를 제공한다. <br>\n",
        "이를 사용하면 미니 배치 학습, 데이터 셔플, 병렬 처리 등을 간단히 수행할 수 있다. <br>\n",
        "\n",
        "기본적인 사용법은 **Dataset을 정의하고, 이를 DataLoader에 전달해야 한다.** <br>\n",
        "Dataset을 커스텀하여 만들 수도 있지만 여기서는 텐서를 입력받아 Dataset의 형태로 변환해주는 TensorDataset을 사용해보자."
      ],
      "metadata": {
        "id": "mNoE22x7jcDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "sqYw1VO0l36e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorDataset은 기본적으로 텐서를 입력 받는다."
      ],
      "metadata": {
        "id": "2tm3DNVcmFxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  90], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "dataset = TensorDataset(x_train, y_train)"
      ],
      "metadata": {
        "id": "Lp28u0ccmK8F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋을 만들었다면 데이터로더를 사용할 수 있다. <br>\n",
        "데이터로더는 기본적으로 **데이터셋, 미니 배치의 크기를 입력 받는다.** <br>\n",
        "또한 Epoch마다 데이터셋을 섞어서 학습되는 순서를 바꾸려면 **shuffle=True**를 인자로 전달하면 된다."
      ],
      "metadata": {
        "id": "0su5vvbKmOc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "for i in list(dataloader):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPiY6kEimvab",
        "outputId": "010764c9-f432-4120-dbef-d168b7f3667f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[93., 88., 93.],\n",
            "        [89., 91., 90.]]), tensor([[185.],\n",
            "        [180.]])]\n",
            "[tensor([[ 73.,  80.,  75.],\n",
            "        [ 96.,  98., 100.]]), tensor([[152.],\n",
            "        [196.]])]\n",
            "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델과 옵티마이저를 선언하자."
      ],
      "metadata": {
        "id": "BPLV28hWnXvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Linear(3, 1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "TD4K184qm2Vb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "미니 배치 경사 하강법을 이용해 학습시키자."
      ],
      "metadata": {
        "id": "jXVgD8TgnkHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_epochs = 2000\n",
        "for epoch in range(nb_epochs+1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        "    \n",
        "    prediction = model(x_train)\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch%500 == 0:\n",
        "      print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        )) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_nvsNtCn1aq",
        "outputId": "9d90713e-298f-4ff8-f72b-50fc54459874"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/2000 Batch 1/3 Cost: 3.062688\n",
            "Epoch    0/2000 Batch 2/3 Cost: 6.417829\n",
            "Epoch    0/2000 Batch 3/3 Cost: 8.883194\n",
            "Epoch  500/2000 Batch 1/3 Cost: 2.795532\n",
            "Epoch  500/2000 Batch 2/3 Cost: 0.970570\n",
            "Epoch  500/2000 Batch 3/3 Cost: 4.740135\n",
            "Epoch 1000/2000 Batch 1/3 Cost: 1.298281\n",
            "Epoch 1000/2000 Batch 2/3 Cost: 1.023259\n",
            "Epoch 1000/2000 Batch 3/3 Cost: 0.625995\n",
            "Epoch 1500/2000 Batch 1/3 Cost: 0.321468\n",
            "Epoch 1500/2000 Batch 2/3 Cost: 1.140725\n",
            "Epoch 1500/2000 Batch 3/3 Cost: 1.080793\n",
            "Epoch 2000/2000 Batch 1/3 Cost: 0.153534\n",
            "Epoch 2000/2000 Batch 2/3 Cost: 0.417696\n",
            "Epoch 2000/2000 Batch 3/3 Cost: 0.742096\n"
          ]
        }
      ]
    }
  ]
}