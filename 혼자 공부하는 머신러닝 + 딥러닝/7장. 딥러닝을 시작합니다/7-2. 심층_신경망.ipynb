{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7-2. 심층 신경망.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOJDRUlIVRrB"
      },
      "source": [
        "1절에서 만들어본 인공신경망의 성능을 더 높여보자. 먼저 데이터를 가져와보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTf042fNqxcB",
        "outputId": "a9e97e0a-ccae-4503-8601-f8a6f84ff993"
      },
      "source": [
        "from tensorflow import keras\n",
        "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWBkNfcVuKD"
      },
      "source": [
        "픽셀값을 0 ~ 1 사이 값으로 변화시키고 일차원 배열로 펼쳐보자.\n",
        "그리고 테스트 세트와 훈련 세트로 나눠보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7uf_0cGrJxR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_scaled = train_input / 255.0\n",
        "train_scaled = train_scaled.reshape(-1, 28 * 28)\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUvXLn8ZV9c0"
      },
      "source": [
        "이번에는 인공 신경망 모델에 층을 2개 추가해보자.\n",
        "\n",
        "1절을 만든 신경망 모델에서 입력층과 출력층 사이에 밀집층을 추가할 것이다. 이렇게 입력층와 출력층 사이에 있는 모든 층을 **'은닉층(hidden layer)'**이라고 부른다.\n",
        "\n",
        "은닉층에는 **활성화 함수가 존재**하며 이 활성화 함수는 신경망 층의 선형 방정식의 계산 값에 적용하는 함수이다. 이전에 출력층에 적용했던 소프트맥스 함수도 활성화 함수이다. 출력층에 적용하는 활성화 함수는 이진 분류일 경우 시그모이드 함수, 다중 분류일 경우 소프트맥수 함수를 사용하는 등 종류가 제한된다. **하지만 은닉층의 활성화 함수는 비교적 자유롭다.**\n",
        "\n",
        "그런데 은닉층에 왜 활성화 함수를 적용할까? 아래 두 선형 방정식을 보자.\n",
        "\n",
        "* a × 4 + 2 = b\n",
        "* b × 3 - 5 = c\n",
        "\n",
        "이때 위 식을 아래의 식에 대입하면 b는 사라진다. 즉, b가 하는 일이 없는 셈이다.\n",
        "\n",
        "* a × 12 + 1 = c\n",
        "\n",
        "신경망도 마찬가지이다. 은닉층에서 선형적인 산술 계산만 수행하면 수행 역할이 없는 셈이다. **선형 계산을 적당하게 비선형적으로 비틀어야 한다.** 그래야 다음 층의 계산과 단순히 합쳐지지 않고 나름의 역할을 수행할 수 있다. 아래의 과정과 비슷하다.\n",
        "\n",
        "* a × 4 + 2 = b\n",
        "* log(b) = k\n",
        "* k × 3 - 5 = c (b를 대입하는 것이 아니라 k를 대입함.)\n",
        "\n",
        "많이 사용되는 활성화 함수중 하나는 '시그모이드 함수'이다. 이 함수는 뉴런의 출력 z값을 0 과 1사이로 압축한다. 그럼 시그모이드 활성화 함수를 사용한 은닉층과 소프트맥스 함수를 사용한 출력층을 케라스의 Dense 클래스로 만들어보자. 신경망의 첫 번째 층은 input_shape 매개변수로 입력의 크기를 지정해야한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IAopA-SvEtA"
      },
      "source": [
        "dense1 = keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784, ))\n",
        "dense2 = keras.layers.Dense(10, activation = 'softmax')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsuS7H-zY6mJ"
      },
      "source": [
        "dense1이 은닉층이고 100개의 뉴런을 가진 밀집층이다. 활성화 함수를 'sigmoid'로 지정했고 input_shape 매개변수에서 입력의 크기를 (784,)로 지정했다. 은닉층의 뉴런 개수를 정하는 데는 특별한 기준이 없다. 하지만 적어도 **출력층의 뉴런보다는 많게 만들어야한다**.\n",
        "\n",
        "이제 두 객체를 Sequential 클래스에 추가하여 **'심층 신경망(deep nueral network, DNN)'**을 만들어 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjXI_vaZvash"
      },
      "source": [
        "model = keras.Sequential([dense1, dense2])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30Qk6hxBZbep"
      },
      "source": [
        "Sequential 클래스의 객체를 만들 때 여러 개의 층을 추가하려면 리스트로 전달하면된다. 단, 출력층을 가장 마지막에 두어야 한다.\n",
        "\n",
        "케라스 모델의 **summary() 메서드**를 호출하면 층에 대한 정보를 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q50aM9nqvr8B",
        "outputId": "c03ce319-d91b-431f-e27d-18fc55e849c9"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a70oGtZnZslJ"
      },
      "source": [
        "맨 첫 줄에 모델의 이름이 나온다. 그 다음 이 모델의 층이 순서대로 출력된다.\n",
        "\n",
        "층 마다 층 이름, 클래스, 모델 파라미터 개수가 출력된다. 층을 만들때 **name 매개변수**로 이름을 지정할 수 있고 지정하지 않으면 자동으로 'dense'라고 이름을 붙인다.\n",
        "\n",
        "출력 크기를 보면 (None, 100)이다. 첫 번째 차원은 **샘플의 개수**를 나타낸다. 샘플의 개수가 아직 정의되지 않아 None이다. 케라스 모델의 fit() 메서드에 훈련 데이터를 주입하면 이 데이터를 **미니배치 경사 하강법으로 훈련**한다.\n",
        "\n",
        "케라스의 **기본 미니배치 크기는 32개**이며 fit() 메서드의 **batch_size 매개변수**로 바꿀 수 있다. 따라서 샘플 개수를 고정하지 않고 어떤 배치 크기에도 유연하게 대응할 수 있도록 None으로 설정한다. 이렇게 신경망 층에 입력되거나 출력되는 배열의 첫 번째 차원을 **배치 차원**이라고 부른다.\n",
        "\n",
        "두 번째 차원은 은닉층의 뉴런 개수를 100개로 두었으니 100개의 출력이 나온다. 즉 샘플마다 784개의 픽셀값이 은닉층을 통과하면서 100개의 특성으로 압축된다.\n",
        "\n",
        "마지막으로 모델 파라미터 개수가 출력된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD7u-sPAb3sB"
      },
      "source": [
        "모델을 훈련하기 전에 Sequential 클래스에 층을 추가하는 다른 방법을 알아보자. 앞에서는 Dense 클래스 객체 dense1과 dense2를 만들어 리스트로 전달했다. 사실 이 두 객체를 따로 저장하여 쓸 일이 없다. 따라서 아래처럼 **Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 경우가 많다**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBMDCbKnwznK"
      },
      "source": [
        "model = keras.Sequential([\n",
        "                          keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,), name = 'hidden'),\n",
        "                          keras.layers.Dense(10, activation = 'softmax', name = 'output')\n",
        "], name = '패션 MNIST 모델')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCI34kbQbtw5"
      },
      "source": [
        "이렇게 작업하면 추가되는 층을 한눈에 쉽게 알아볼 수 있다. summary() 메서드를 사용해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZZMQJEC_0m",
        "outputId": "95164af6-10b3-4917-f621-950f38894515"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"패션 MNIST 모델\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden (Dense)               (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiqpYNr8mHfg"
      },
      "source": [
        "이전과 동일한 수치들이 나왔고 name 매개변수에 전달한 이름 역시 올바르게 출력되었다. \n",
        "\n",
        "이 방법도 편리하지만 아주 많은 층을 추가하려면 Sequential 클래스 생성자가 너무 길어진다. Sequential 클래스에서 층을 추가할 때는 모델의 **add() 메서드**를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHhflbI0DCH5"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(100, activation = 'sigmoid', input_shape = (784,)))\n",
        "model.add(keras.layers.Dense(10, activation = 'softmax'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7mvVKXmmmAJ"
      },
      "source": [
        "summary() 메서드를 사용해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8meMbzADjyq",
        "outputId": "6041760f-18a5-4bcd-b035-a6728cc8b44a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08XTgx_4mqzh"
      },
      "source": [
        "모델을 훈련해보자. compile() 메서드의 설정은 1절과 동일하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rQ_oOd-Dmjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660e9edb-69f9-4a5d-c8a1-137bf27318c2"
      },
      "source": [
        "model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
        "model.fit(train_scaled, train_target, epochs = 5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5628 - accuracy: 0.8069\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4075 - accuracy: 0.8532\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3731 - accuracy: 0.8654\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3496 - accuracy: 0.8727\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3323 - accuracy: 0.8796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fea8f643ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA5NTa8umx87"
      },
      "source": [
        "훈련세트의 성능을 보면 추가된 층이 성능을 향상시킨 것을 확인할 수 있다. 인공 신경망에 몇 개의 층을 추가해도 compile() 메서드와 fit() 메서드의 사용법은 동일하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbV9W2Okm9Ph"
      },
      "source": [
        "이번에는 이미지 분류 문제에서 높은 성능을 낼 수 있는 활성화 함수에 대해 알아보자. \n",
        "\n",
        "초창기 인공 신경망의 은닉층에 많이 사용된 활성화 함수는 시그모이드 함수였다. 하지만 이 함수는 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응하지 못한다.\n",
        "\n",
        "특히 층이 많은 심층 신경망일수록 그 효과가 누적되어 학습을 더 어렵게한다. 이를 위해 **'렐루(ReLU) 함수'**가 제안되었다. 렐루 함수는 입력이 양수일 경우 활성화 함수가 없는 것처럼 입력을 통과시키고 음수일 경우 0으로 만든다.\n",
        "\n",
        "렐루 함수는 **max(0, z)**로 쓸 수 있다. z가 0보다 크면 z를 출력하고 0보다 작으면 0을 출력한다. 렐루 함수는 특히 이미지 처리에서 좋은 성능을 낸다고 알려져있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0IPxOZooSlp"
      },
      "source": [
        "렐루 함수를 사용하기전 케라스에서 제공하는 기능 하나를 살펴보자.\n",
        "\n",
        "패션 MNIST 데이터는 28 × 28 크기 이므로 인공 신경망에 주입하기 위해 넘파이 배열의 reshape()메서드를 사용해 1차원으로 펼쳤다. 직접 이렇게 1차원으로 펼쳐도 좋지만 케라스에서는 이를 위한 **Flatten 층**을 제공한다.\n",
        "\n",
        "Flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할만 한다. 즉, 인공 신경망의 성능을 위해 기여하는 것은 없다.\n",
        "\n",
        "Flatten 층과 렐루 함수를 사용해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA4VwRDE-iHE"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape = (28, 28)))\n",
        "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
        "model.add(keras.layers.Dense(10, activation = 'softmax'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du_EoZzkpJxK"
      },
      "source": [
        "summary() 메서드를 호출해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw94EG-5Aiwn",
        "outputId": "449760af-f55a-48cb-c885-d20262a92b73"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnsyYO23pdaK"
      },
      "source": [
        "Flatten 클래스에 포함된 모델 파라미터는 0개이다. 케라스의 Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있다. 즉, 앞의 출력에서 784개의 입력이 첫 번째 은닉층에 전달된다는 것을 알 수 있다.\n",
        "\n",
        "훈련 데이터를 다시 준비해서 모델을 훈련해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYHIei8KAtIL"
      },
      "source": [
        "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n",
        "train_scaled = train_input / 255.0\n",
        "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IyWVcTNBOR8",
        "outputId": "1fbdc1b0-6bf1-43c4-c53e-4804e024b3d9"
      },
      "source": [
        "model.compile(loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
        "model.fit(train_scaled, train_target, epochs = 5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.5353 - accuracy: 0.8115\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3932 - accuracy: 0.8598\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3545 - accuracy: 0.8725\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3335 - accuracy: 0.8798\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3199 - accuracy: 0.8865\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fea8f48a610>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3aCwa1Kp4BR"
      },
      "source": [
        "시그모이드 함수를 사용했을 때보다 성능이 조금 향상되었다. 검증 세트에서의 성능도 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxyM0plhBbp9",
        "outputId": "b9d226d9-a1a1-47f7-d5a8-043470a4c541"
      },
      "source": [
        "model.evaluate(val_scaled, val_target)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 1s 1ms/step - loss: 0.3642 - accuracy: 0.8758\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3641807436943054, 0.8758333325386047]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UbtnRsIqArV"
      },
      "source": [
        "1절에서 은닉층을 추가하지 않은 경우보다 성능이 향상되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNxig_eNqJoK"
      },
      "source": [
        "추가적으로 인공 신경망의 하이퍼파라미터에 대해 알아보자.\n",
        "\n",
        "하이퍼파라미터란 모델이 학습하지 않아 사람이 지정해주어야 하는 파라미터이다. 이번 절에서 어떤 하이퍼파라미터가 있었는지 생각해보자.\n",
        "\n",
        "* 은닉층의 개수\n",
        "* 은닉층의 뉴런 개수\n",
        "* 활성화 함수\n",
        "* 층의 종류\n",
        "* batch_size 매개변수\n",
        "* epochs 매개변수\n",
        "* compile() 메서드의 RMSprop\n",
        "\n",
        "compile() 메서드에서는 케라스의 기본 경사 하강법 알고리즘인 **RMSprop**을 사용했다. 케라스는 다양한 종류의 경사 하강법 알고리즘을 제공하며 이들을 **'옵티마이저(optimizer)'**라고 부른다. 위 하이퍼파라미터 외에도 RMSprop의 학습률 또한 조정할 하이퍼파라미터 중 하나이다.\n",
        "\n",
        "즉, 처음부터 모델을 구성하고 각종 하이퍼파라미터의 최적값을 찾는 것은 어려운 작업이다. 이번에는 여러가지 옵티마이저를 테스트해보자. 가장 기본적인 옵티마이저는 확률적 경사 하강법인 **SGD**이다. 이름은 SGD이지만 기본적으로 미니배치를 사용한다.\n",
        "\n",
        "SGD 옵티마이저를 사용하려면 compile()메서드의 **optimizer 매개변수를 'sgd'로 지정**하면 된다.\n",
        "\n",
        "* model.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
        "\n",
        "이 옵티마이저는 **tensorflow.keras.optimizers 패키지 아래 SGD 클래스**로 구현되어있다. 'sgd' 문자열은 이 클래스의 기본 설정 매개변수로 생성한 객체와 동일하다. 즉 다음 코드는 위와 동일하다.\n",
        "\n",
        "* sgd = keras.optimizers.SGD()\n",
        "* model.compile(optimizer = sgd, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
        "\n",
        "만약 SGD 클래스의 학습률 기본값이 0.01일 때 이를 바꾸고 싶다면 **learning_rate 매개변수**에 학습률을 지정해야한다.\n",
        "\n",
        "* sgd = keras.optimizers.SGD(learning_rate = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aAhzriLt8EV"
      },
      "source": [
        "SGD 외에도 다양한 옵티마이저들이 있다. 기본 경사 하강법 옵티마이저는 모두 SGD 클래스에서 제공한다. SGD 클래스의 momentum 매개변수의 기본값은 0이다. 이를 0보다 큰 값으로 지정하면 이전의 그레이디언트를 가속도처럼 사용하는 **'모멘텀 최적화(momentum optimization)'**를 사용한다. 보통 momentum 매개변수는 **0.9 이상**을 지정한다. SGD 클래스의 **nesterov 매개변수**를 기본값 False에서 True로 바꾸면 **'네스테로프 모멘텀 최적화(nesterov momentum optimization)'**를 사용한다.\n",
        "\n",
        "* sgd = keras.optimizers.SGD(momentum = 0.9, nesterov = True)\n",
        "\n",
        "네스테로프 모멘텀은 모멘텀 최적화를 2번 반복하여 구현한다. 일반적으로 네스테로프 모멘텀 최적화가 기본 확률적 경사 하강법보다 더 나은 성능을 제공한다.\n",
        "\n",
        "모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있다. 이렇게 하면 안정적으로 최적점에 수렴할 가능성이 높다. 이런 학습률을 **'적응적 학습률(adaptive learning rate)'**라고 한다. 이런 방식들은 학습률 매개변수를 튜닝하는 수고를 덜 수 있다.\n",
        "\n",
        "적응적 학습률을 제공하는 대표적인 옵티마이저는 Adagrad와 RMSprop이다. 각각 compile() 메서드의 optimizer 매개변수에 **'adagard'**와 **'rmsprop'**로 지정할 수 있다. optimizer 매개변수의 기본값이 'rmsprop'이다. 이 두 옵티마이저의 매개변수를 바꾸고 싶다면 SGD와 같이 Adagrad와 RMSprop 클래스 객체를 만들어 사용하면 된다.\n",
        "\n",
        "* adagrad = keras.optimizers.Adagrad()\n",
        "* model.compile(optimizer = adagrad, loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
        "\n",
        "RMS도 동일한 방식으로 사용할 수 있다.\n",
        "\n",
        "모멘텀 최적화와 RMSprop의 장점을 접목한 것이 Adam이다. Adam은 RMSprop와 함께 처음 시도해 볼 수 있는 좋은 알고리즘이다. 적응적 합습률을 사용하는 이 3개의 클래스의 learning_rate 매개변수 기본값은 0.001이다.\n",
        "\n",
        "Adam 클래스의 기본값을 사용해 패션 MNIST 모델을 훈련해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9ptpxGQBogL"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape = (28, 28)))\n",
        "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
        "model.add(keras.layers.Dense(10, activation = 'softmax'))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcXOOXseyRJ8",
        "outputId": "3bf50982-2f13-420a-ded3-82792b99c00e"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')\n",
        "model.fit(train_scaled, train_target, epochs = 5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3936 - accuracy: 0.8586\n",
            "Epoch 2/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3502 - accuracy: 0.8740\n",
            "Epoch 3/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3251 - accuracy: 0.8816\n",
            "Epoch 4/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3035 - accuracy: 0.8891\n",
            "Epoch 5/5\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2917 - accuracy: 0.8925\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fea8f1555d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1n3aYPPydPO"
      },
      "source": [
        "RMSprop를 사용했을 때와 거의 같은 성능을 보여준다. 검증 세트에서의 성능도 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFSEUxbRycLc",
        "outputId": "b583997c-f19a-46d5-c023-803b86e9ec0e"
      },
      "source": [
        "model.evaluate(val_scaled, val_target)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "375/375 [==============================] - 1s 1ms/step - loss: 0.3312 - accuracy: 0.8823\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33124950528144836, 0.8823333382606506]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQUznK3Yyui7"
      },
      "source": [
        "# 정리\n",
        "\n",
        "핵심 포인트\n",
        "* 심층 신경망: **2개 이상의 층을 포함한 신경망이다.** 종종 다층 인공 신경망, 심층 신경망, 딥러닝을 같은 의미로 사용한다.\n",
        "* 렐루 함수: **이미지 분류 모델의 은닉층에서 많이 사용하는 활성화 함수이다.** 시그모이드 함수는 층이 많을수록 활성화 함수의 양쪽 끝에서 변화가 작기 때문에 학습이 어려워진다. 렐루 함수는 이런 문제가 없으며 계산도 간단하다.\n",
        "* 옵티마이저: **신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법**을 말한다. 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있다. 대표적으로 SGD, 모멘텀, RMSprop, Adam 등이 있다.\n",
        "\n",
        "핵심 패키지와 함수\n",
        "\n",
        "TensorFlow\n",
        "* add(): **케라스 모델에서 층을 추가하는 메서드**이다.\n",
        "* summary(): **케라스 모델의 정보를 출력하는 메서드**이다.\n",
        "* SGD: **기본 경사 하강법 옵티마이저 클래스**이다. **learning_rate 매개변수**로 학습률을 지정하며 기본값은 0.01이다. **momentum 매개변수**에 0 이상의 값을 지정하면 모멘텀 최적화를 수행한다. **nesterov 매개변수**를 True로 설정하면 네스테로프 모멘텀 최적화를 수행한다.\n",
        "* Adagrad: **Adagrad 옵티마이저 클래스**이다. **learning_rate 매개변수**로 학습률을 지정하며 기본값은 0.001이다. Adagard는 그레이디언트 제곱을 누적하여 학습률을 나눈다. **initial_accumulator_value 매개변수**에서 누적 초깃값을 지정할 수 있으며 기본값은 0.1이다.\n",
        "* RMSprop: **RMSprop 옵티마이저 클래스**이다. **learning_rate 매개변수**로 학습률을 지정하며 기본값은 0.001이다. Adagrad처럼 그레이디언트 제곱으로 학습률을 나누지만 최근의 그레이디언트를 사용하기위해 지수 감소를 사용한다. **rho 매개변수**에서 감소 비율을 지정하며 기본값은 0.9이다.\n",
        "* Adam: **Adam 옵티마이저 클래스**이다. **learning_rate 매개변수**로 학습률을 지정하며 기본값은 0.001이다. 모멘텀 최적화에 있는 그레이디언트의 지수 감소 평균을 조절하기 위해 **beta_1 매개변수**가 있으며 기본값은 0.9이다. RMSprop에 있는 그레이디언트 제곱의 지수 감소 평균을 조절하기 위해 **beta_2 매개변수**가 있으며 기본값은 0.999이다."
      ]
    }
  ]
}